{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0168229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL to fetch the schedule from\n",
    "url = 'https://www.insidelacrosse.com/league/di/calendar'\n",
    "\n",
    "# Fetch the webpage content\n",
    "response = requests.get(url)\n",
    "html_content = response.content  # Use .text if the response is text/html\n",
    "\n",
    "# Load the HTML content into BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "games_list = []\n",
    "\n",
    "# Adjusted selector to accurately target game rows, assuming a structure (you might need to update this based on actual HTML structure)\n",
    "for game_row in soup.find_all('tr', class_='game-row-class'):  # Update 'game-row-class' with the actual class if needed\n",
    "    # Extracting the team names, times, and watch links where available\n",
    "    teams = game_row.find_all('a')[:2]  # Assuming the first two 'a' tags are team names\n",
    "    time = game_row.find('td', class_='time-class')  # Update 'time-class' with the actual class of the 'td' containing time\n",
    "    watch_link_tag = game_row.find('a', text='Webstream')\n",
    "    \n",
    "    if teams and time:  # Ensuring there's enough info to consider it a valid game row\n",
    "        game_details = {\n",
    "            'home_team': teams[0].text.strip(),\n",
    "            'away_team': teams[1].text.strip(),\n",
    "            'time': time.text.strip(),\n",
    "            'watch_link': watch_link_tag['href'] if watch_link_tag else 'Not available'\n",
    "        }\n",
    "        games_list.append(game_details)\n",
    "\n",
    "# Display the extracted game details\n",
    "for game in games_list:\n",
    "    print(f\"Home Team: {game['home_team']}, Away Team: {game['away_team']}, Time: {game['time']}, Watch: {game['watch_link']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d05e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Setup Selenium WebDriver\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode, without a UI\n",
    "driver = webdriver.Chrome(executable_path='path/to/chromedriver', options=chrome_options)\n",
    "\n",
    "# URL of the lacrosse game schedule\n",
    "url = 'https://www.insidelacrosse.com/league/di/calendar'\n",
    "\n",
    "# Navigate to the page\n",
    "driver.get(url)\n",
    "\n",
    "# Scroll to the bottom of the page to trigger dynamic content load\n",
    "# Note: This is a simplistic approach; adjust the scrolling as needed based on page structure\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "# Wait for the dynamic content to load\n",
    "time.sleep(5)  # Adjust this delay as necessary\n",
    "\n",
    "# Now that the page is fully loaded, get the page source\n",
    "html_content = driver.page_source\n",
    "driver.quit()\n",
    "\n",
    "# Parse the loaded HTML content with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Continue with BeautifulSoup to parse the game schedule as previously discussed\n",
    "# Your parsing code goes here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30a83c64",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WebDriver.__init__() got an unexpected keyword argument 'executable_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 59\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHome Team: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhome_team\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Away Team: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maway_team\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgame[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Watch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgame\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwatch_link\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNot available\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 59\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 51\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[1;32m     50\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.insidelacrosse.com/league/di/calendar\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 51\u001b[0m     html_content \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_dynamic_content\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     games_list \u001b[38;5;241m=\u001b[39m parse_page(html_content)\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Display the extracted game details\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m, in \u001b[0;36mfetch_dynamic_content\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m     10\u001b[0m chrome_options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--headless\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Run in headless mode\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Replace 'path/to/chromedriver' with the path to the WebDriver executable\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath/to/chromedriver\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchrome_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Navigate to the page\u001b[39;00m\n\u001b[1;32m     15\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(url)\n",
      "\u001b[0;31mTypeError\u001b[0m: WebDriver.__init__() got an unexpected keyword argument 'executable_path'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def fetch_dynamic_content(url):\n",
    "    # Setup Selenium WebDriver\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "    # Replace 'path/to/chromedriver' with the path to the WebDriver executable\n",
    "    driver = webdriver.Chrome(executable_path='path/to/chromedriver', options=chrome_options)\n",
    "\n",
    "    # Navigate to the page\n",
    "    driver.get(url)\n",
    "\n",
    "    # Scroll to the bottom to trigger the loading of dynamic content\n",
    "    # Adjust the scrolling as necessary based on the page's loading behavior\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Wait for the dynamic content to load\n",
    "\n",
    "    # Get the fully loaded page source\n",
    "    html_content = driver.page_source\n",
    "    driver.quit()\n",
    "    return html_content\n",
    "\n",
    "def parse_page(html_content):\n",
    "    # Parse the loaded HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    games_list = []\n",
    "    # Assuming each game is contained within a 'tr' element, similar to the provided HTML structure\n",
    "    for game_row in soup.find_all('tr'):\n",
    "        teams = game_row.find_all('td')[:2]  # Assuming the first two 'td' elements contain the teams\n",
    "        time = game_row.find('td', text=True, recursive=False)  # Game time\n",
    "        watch_link_tag = game_row.find('a', href=True, text=lambda x: x and \"Webstream\" in x)\n",
    "\n",
    "        if teams and time:\n",
    "            game_details = {\n",
    "                'home_team': teams[0].text.strip(),\n",
    "                'away_team': teams[1].text.strip(),\n",
    "                'time': time.text.strip(),\n",
    "                'watch_link': watch_link_tag['href'] if watch_link_tag else 'Not available'\n",
    "            }\n",
    "            games_list.append(game_details)\n",
    "\n",
    "    return games_list\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.insidelacrosse.com/league/di/calendar'\n",
    "    html_content = fetch_dynamic_content(url)\n",
    "    games_list = parse_page(html_content)\n",
    "\n",
    "    # Display the extracted game details\n",
    "    for game in games_list:\n",
    "        print(f\"Home Team: {game['home_team']}, Away Team: {game['away_team']}, Time: {game['time']}, Watch: {game.get('watch_link', 'Not available')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ba519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "def fetch_dynamic_content(url):\n",
    "    # Setup Selenium WebDriver with updated syntax\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "\n",
    "    # Automatically manage driver\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "    # Your code continues from here...\n",
    "    # Navigate to the page\n",
    "    driver.get(url)\n",
    "\n",
    "    # Scroll to the bottom to trigger the loading of dynamic content\n",
    "    # Adjust the scrolling as necessary based on the page's loading behavior\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(5)  # Wait for the dynamic content to load\n",
    "\n",
    "    # Get the fully loaded page source\n",
    "    html_content = driver.page_source\n",
    "    driver.quit()\n",
    "    return html_content\n",
    "\n",
    "def parse_page(html_content):\n",
    "    # Parse the loaded HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    games_list = []\n",
    "    # Assuming each game is contained within a 'tr' element, similar to the provided HTML structure\n",
    "    for game_row in soup.find_all('tr'):\n",
    "        teams = game_row.find_all('td')[:2]  # Assuming the first two 'td' elements contain the teams\n",
    "        time = game_row.find('td', text=True, recursive=False)  # Game time\n",
    "        watch_link_tag = game_row.find('a', href=True, text=lambda x: x and \"Webstream\" in x)\n",
    "\n",
    "        if teams and time:\n",
    "            game_details = {\n",
    "                'home_team': teams[0].text.strip(),\n",
    "                'away_team': teams[1].text.strip(),\n",
    "                'time': time.text.strip(),\n",
    "                'watch_link': watch_link_tag['href'] if watch_link_tag else 'Not available'\n",
    "            }\n",
    "            games_list.append(game_details)\n",
    "\n",
    "    return games_list\n",
    "\n",
    "def main():\n",
    "    url = 'https://www.insidelacrosse.com/league/di/calendar'\n",
    "    html_content = fetch_dynamic_content(url)\n",
    "    games_list = parse_page(html_content)\n",
    "\n",
    "    # Display the extracted game details\n",
    "    for game in games_list:\n",
    "        print(f\"Home Team: {game['home_team']}, Away Team: {game['away_team']}, Time: {game['time']}, Watch: {game.get('watch_link', 'Not available')}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d89e7796",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = fetch_dynamic_content(url)\n",
    "with open(\"page_content.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(html_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d556a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to retrieve content. Status code: 403\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the URL you want to download from\n",
    "url = \"https://www.insidelacrosse.com/league/di/calendar\"\n",
    "\n",
    "# Specify the path to save the HTML content\n",
    "filepath = \"example.html\"\n",
    "\n",
    "# Make the HTTP GET request to fetch the content from the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Write the content to the specified file\n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(response.text)\n",
    "    print(f\"HTML content has been saved to {filepath}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve content. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27a32210",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WebDriver.__init__() got multiple values for argument 'options'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m chrome_options\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--headless\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Indicate the path to your ChromeDriver and pass in the options\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m driver \u001b[38;5;241m=\u001b[39m \u001b[43mwebdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mChrome\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/opt/homebrew/bin/chromedriver\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchrome_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Navigate to Google's homepage\u001b[39;00m\n\u001b[1;32m     12\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp://www.google.com\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: WebDriver.__init__() got multiple values for argument 'options'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set options to enable headless mode\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "\n",
    "# Indicate the path to your ChromeDriver and pass in the options\n",
    "driver = webdriver.Chrome('/opt/homebrew/bin/chromedriver', options=chrome_options)\n",
    "\n",
    "# Navigate to Google's homepage\n",
    "driver.get('http://www.google.com')\n",
    "\n",
    "# Find the search box element by its name\n",
    "search_box = driver.find_element_by_name('q')\n",
    "\n",
    "# Type a query in the search box\n",
    "search_box.send_keys('Hello, Selenium!')\n",
    "\n",
    "# Submit the query\n",
    "search_box.submit()\n",
    "\n",
    "# Pause the script for 5 seconds to get the results, then print the page title and close the browser\n",
    "import time\n",
    "time.sleep(5)\n",
    "print(driver.title)\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2a1d5f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchElementException",
     "evalue": "Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[name=\"my-text\"]\"}\n  (Session info: chrome=121.0.6167.184); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n0   chromedriver                        0x0000000104c667dc chromedriver + 4040668\n1   chromedriver                        0x0000000104c5e9e0 chromedriver + 4008416\n2   chromedriver                        0x00000001048d1870 chromedriver + 284784\n3   chromedriver                        0x0000000104915080 chromedriver + 561280\n4   chromedriver                        0x000000010494f048 chromedriver + 798792\n5   chromedriver                        0x000000010490974c chromedriver + 513868\n6   chromedriver                        0x000000010490a044 chromedriver + 516164\n7   chromedriver                        0x0000000104c2ba04 chromedriver + 3799556\n8   chromedriver                        0x0000000104c2fee4 chromedriver + 3817188\n9   chromedriver                        0x0000000104c14260 chromedriver + 3703392\n10  chromedriver                        0x0000000104c30a2c chromedriver + 3820076\n11  chromedriver                        0x0000000104c0701c chromedriver + 3649564\n12  chromedriver                        0x0000000104c4de3c chromedriver + 3939900\n13  chromedriver                        0x0000000104c4dfb4 chromedriver + 3940276\n14  chromedriver                        0x0000000104c5e660 chromedriver + 4007520\n15  libsystem_pthread.dylib             0x000000018b8b2034 _pthread_start + 136\n16  libsystem_pthread.dylib             0x000000018b8ace3c thread_start + 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchElementException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m title \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mtitle\n\u001b[1;32m     10\u001b[0m driver\u001b[38;5;241m.\u001b[39mimplicitly_wait(\u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m text_box \u001b[38;5;241m=\u001b[39m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmy-text\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m submit_button \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_element(by\u001b[38;5;241m=\u001b[39mBy\u001b[38;5;241m.\u001b[39mCSS_SELECTOR, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbutton\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m text_box\u001b[38;5;241m.\u001b[39msend_keys(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelenium\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:741\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    738\u001b[0m     by \u001b[38;5;241m=\u001b[39m By\u001b[38;5;241m.\u001b[39mCSS_SELECTOR\n\u001b[1;32m    739\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/site-packages/selenium/webdriver/remote/webdriver.py:347\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    345\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_executor\u001b[38;5;241m.\u001b[39mexecute(driver_command, params)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m--> 347\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_handler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m     response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_unwrap_value(response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.1/lib/python3.12/site-packages/selenium/webdriver/remote/errorhandler.py:229\u001b[0m, in \u001b[0;36mErrorHandler.check_response\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m    227\u001b[0m         alert_text \u001b[38;5;241m=\u001b[39m value[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malert\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[0;31mNoSuchElementException\u001b[0m: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[name=\"my-text\"]\"}\n  (Session info: chrome=121.0.6167.184); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#no-such-element-exception\nStacktrace:\n0   chromedriver                        0x0000000104c667dc chromedriver + 4040668\n1   chromedriver                        0x0000000104c5e9e0 chromedriver + 4008416\n2   chromedriver                        0x00000001048d1870 chromedriver + 284784\n3   chromedriver                        0x0000000104915080 chromedriver + 561280\n4   chromedriver                        0x000000010494f048 chromedriver + 798792\n5   chromedriver                        0x000000010490974c chromedriver + 513868\n6   chromedriver                        0x000000010490a044 chromedriver + 516164\n7   chromedriver                        0x0000000104c2ba04 chromedriver + 3799556\n8   chromedriver                        0x0000000104c2fee4 chromedriver + 3817188\n9   chromedriver                        0x0000000104c14260 chromedriver + 3703392\n10  chromedriver                        0x0000000104c30a2c chromedriver + 3820076\n11  chromedriver                        0x0000000104c0701c chromedriver + 3649564\n12  chromedriver                        0x0000000104c4de3c chromedriver + 3939900\n13  chromedriver                        0x0000000104c4dfb4 chromedriver + 3940276\n14  chromedriver                        0x0000000104c5e660 chromedriver + 4007520\n15  libsystem_pthread.dylib             0x000000018b8b2034 _pthread_start + 136\n16  libsystem_pthread.dylib             0x000000018b8ace3c thread_start + 8\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.insidelacrosse.com/league/di/calendar\")\n",
    "\n",
    "title = driver.title\n",
    "\n",
    "driver.implicitly_wait(0.5)\n",
    "\n",
    "text_box = driver.find_element(by=By.NAME, value=\"my-text\")\n",
    "submit_button = driver.find_element(by=By.CSS_SELECTOR, value=\"button\")\n",
    "\n",
    "text_box.send_keys(\"Selenium\")\n",
    "submit_button.click()\n",
    "\n",
    "message = driver.find_element(by=By.ID, value=\"message\")\n",
    "text = message.text\n",
    "\n",
    "print(title)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c18cd43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "driver.get(\"https://www.insidelacrosse.com/league/di/calendar\")\n",
    "\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "# Save the page source to a file\n",
    "with open(\"page_source.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(driver.page_source)\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b3e57b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "driver.get(\"https://www.insidelacrosse.com/league/di/calendar\")\n",
    "\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "# Save the page source to a file\n",
    "with open(\"page_source.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(driver.page_source)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57d03e86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'page_source.html'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "# Initialize the Chrome driver with the configured options\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Navigate to the specified URL\n",
    "driver.get(\"https://www.insidelacrosse.com/league/di/calendar\")\n",
    "\n",
    "# Wait for the page elements to load\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "# Get the page source\n",
    "page_source = driver.page_source\n",
    "\n",
    "# Specify the file path and name\n",
    "file_path = 'page_source.html'\n",
    "\n",
    "# Write the page source to a file\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(page_source)\n",
    "\n",
    "# Close the browser and end the session\n",
    "driver.quit()\n",
    "\n",
    "# Return the path to the saved file\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8917bdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "driver.get(\"https://www.insidelacrosse.com/league/di/calendar\")\n",
    "\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "with open(\"page_source.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(driver.page_source)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2869a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # Disable GPU (useful for headless)\n",
    "\n",
    "# Mimic a user-agent of a well-known browser\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\")\n",
    "\n",
    "# Initialize the WebDriver with the specified options\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Your target URL\n",
    "driver.get(\"https://www.insidelacrosse.com/league/di/calendar\")\n",
    "\n",
    "# Wait for the page to load (adjust the wait time as necessary)\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "# Perform any actions, such as extracting information from the page\n",
    "# Example: print(driver.title)\n",
    "\n",
    "# Save the page source to a file\n",
    "with open(\"page_source.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(driver.page_source)\n",
    "\n",
    "# Clean up by closing the browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ff6d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6cecd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b272c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'headers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m                     row_data \u001b[38;5;241m=\u001b[39m [col\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols] \u001b[38;5;241m+\u001b[39m [game_date]\n\u001b[1;32m     27\u001b[0m                     all_games_data\u001b[38;5;241m.\u001b[39mappend(row_data)\n\u001b[0;32m---> 29\u001b[0m df_games_with_dates \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_games_data, columns\u001b[38;5;241m=\u001b[39m\u001b[43mheaders\u001b[49m)\n\u001b[1;32m     30\u001b[0m df_games_with_dates\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/all_games_with_dates_extended.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAll games with dates across all days saved to all_games_with_dates_extended.csv.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'headers' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('./data/page_source.html', 'r') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "all_games_data = []\n",
    "\n",
    "# Find all 'h4' tags, each representing a day's header\n",
    "date_tags = soup.find_all('h4')\n",
    "\n",
    "for date_tag in date_tags:\n",
    "    game_date = date_tag.text.strip()\n",
    "    # For each date, find all subsequent tables until the next 'h4' tag\n",
    "    for sibling in date_tag.find_next_siblings():\n",
    "        if sibling.name == 'h4':\n",
    "            break  # Stop if the next 'h4' (date) tag is found\n",
    "        if sibling.name == 'table':\n",
    "            table = sibling\n",
    "            headers = [header.text.strip() for header in table.find_all('th')] + ['Date']\n",
    "            for row in table.find_all('tr'):\n",
    "                cols = row.find_all('td')\n",
    "                if cols:\n",
    "                    row_data = [col.text.strip() for col in cols] + [game_date]\n",
    "                    all_games_data.append(row_data)\n",
    "\n",
    "df_games_with_dates = pd.DataFrame(all_games_data, columns=headers)\n",
    "df_games_with_dates.to_csv('./data/all_games_with_dates_extended.csv', index=False)\n",
    "\n",
    "print('All games with dates across all days saved to all_games_with_dates_extended.csv.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7accb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6719488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9e9358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506fc1bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51dc6ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06897c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539adb2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc03c8b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.insidelacrosse.com/league/di/teams/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec5a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "driver.get(\"https://www.insidelacrosse.com/league/di/teams/2024\")\n",
    "\n",
    "driver.implicitly_wait(5)\n",
    "\n",
    "with open(\"./data/raw_teams.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "    file.write(driver.page_source)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a04c6d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to games.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the HTML content\n",
    "with open('data/raw_teams.html', 'r') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the specific table in the HTML based on the screenshot provided\n",
    "# Assuming the class \"table-striped\" is unique to this table\n",
    "table = soup.find('table', class_='table-striped')\n",
    "\n",
    "# Extract the table headers, skipping the first header row as it is a category header\n",
    "headers = [header.get('original-title', header.text).strip() for header in table.find_all('th')[1:]]\n",
    "\n",
    "# Extract rows, starting from the second row to skip the header row\n",
    "rows = []\n",
    "for row in table.find_all('tr')[1:]:  # skipping the first row which is the header\n",
    "    cols = row.find_all('td')\n",
    "    if cols:\n",
    "        rows.append([col.text.strip() for col in cols])\n",
    "\n",
    "# Create a DataFrame and save to CSV\n",
    "df = pd.DataFrame(rows, columns=headers)\n",
    "csv_path = 'data/teams.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print('Data saved to games.csv.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25e23238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Team,W,L,G,GA,DIFF,GPP\r\n",
      "NJIT,4,0,51,43,8,12.750\r\n",
      "Ohio State,4,0,46,24,22,11.500\r\n",
      "Syracuse,4,1,84,40,44,16.800\r\n",
      "Denver,3,0,40,29,11,13.333\r\n",
      "Duke,3,0,60,31,29,20.000\r\n",
      "Maryland,3,0,36,27,9,12.000\r\n",
      "Boston U,3,1,54,40,14,13.500\r\n",
      "High Point,3,1,59,45,14,14.750\r\n",
      "Hofstra,3,1,60,50,10,15.000\r\n",
      "Johns Hopkins,3,1,49,34,15,12.250\r\n",
      "Marquette,3,1,58,40,18,14.500\r\n",
      "Colgate,3,2,61,72,-11,12.200\r\n",
      "Army,2,0,29,19,10,14.500\r\n",
      "Delaware,2,0,31,12,19,15.500\r\n",
      "Harvard,2,0,37,20,17,18.500\r\n",
      "North Carolina,2,0,32,13,19,16.000\r\n",
      "Notre Dame,2,0,46,11,35,23.000\r\n",
      "Princeton,2,0,30,11,19,15.000\r\n",
      "Quinnipiac,2,0,30,18,12,15.000\r\n",
      "Virginia,2,0,33,21,12,16.500\r\n",
      "Yale,2,0,34,22,12,17.000\r\n",
      "Bryant,2,1,40,33,7,13.333\r\n",
      "Holy Cross,2,1,32,30,2,10.667\r\n",
      "Lafayette,2,1,36,23,13,12.000\r\n",
      "Marist,2,1,37,36,1,12.333\r\n",
      "Merrimack,2,1,33,30,3,11.000\r\n",
      "Michigan,2,1,50,32,18,16.667\r\n",
      "Navy,2,1,40,31,9,13.333\r\n",
      "Penn State,2,1,44,33,11,14.667\r\n",
      "Rutgers,2,1,37,40,-3,12.333\r\n",
      "Towson,2,1,37,23,14,12.333\r\n",
      "UMass,2,1,40,31,9,13.333\r\n",
      "VMI,2,2,59,47,12,14.750\r\n",
      "Cornell,1,0,17,13,4,17.000\r\n",
      "Le Moyne,1,0,17,6,11,17.000\r\n",
      "Binghamton,1,1,21,21,0,10.500\r\n",
      "Drexel,1,1,18,18,0,9.000\r\n",
      "Jacksonville,1,1,17,17,0,8.500\r\n",
      "LIU,1,1,20,22,-2,10.000\r\n",
      "Penn,1,1,20,19,1,10.000\r\n",
      "Robert Morris,1,1,23,34,-11,11.500\r\n",
      "Bucknell,1,2,35,44,-9,11.667\r\n",
      "Canisius,1,2,22,41,-19,7.333\r\n",
      "Cleveland State,1,2,21,49,-28,7.000\r\n",
      "Georgetown,1,2,31,38,-7,10.333\r\n",
      "Hobart,1,2,32,43,-11,10.667\r\n",
      "Lehigh,1,2,33,38,-5,11.000\r\n",
      "Loyola,1,2,29,34,-5,9.667\r\n",
      "Mercer,1,2,32,32,0,10.667\r\n",
      "Monmouth,1,2,32,39,-7,10.667\r\n",
      "Providence,1,2,32,44,-12,10.667\r\n",
      "Richmond,1,2,44,33,11,14.667\r\n",
      "Stony Brook,1,2,39,44,-5,13.000\r\n",
      "Vermont,1,2,27,47,-20,9.000\r\n",
      "Bellarmine,1,3,38,47,-9,9.500\r\n",
      "Sacred Heart,1,3,46,47,-1,11.500\r\n",
      "Iona,0,0,0,0,0,0.000\r\n",
      "Dartmouth,0,1,13,15,-2,13.000\r\n",
      "Detroit Mercy,0,1,8,22,-14,8.000\r\n",
      "Lindenwood,0,1,5,16,-11,5.000\r\n",
      "UMBC,0,1,10,11,-1,10.000\r\n",
      "Brown,0,2,17,29,-12,8.500\r\n",
      "Queens,0,2,18,31,-13,9.000\r\n",
      "Saint Joseph's,0,2,20,33,-13,10.000\r\n",
      "Siena,0,2,27,30,-3,13.500\r\n",
      "UAlbany,0,2,17,22,-5,8.500\r\n",
      "UMass Lowell,0,2,16,26,-10,8.000\r\n",
      "Villanova,0,2,21,31,-10,10.500\r\n",
      "Wagner,0,2,15,38,-23,7.500\r\n",
      "Air Force,0,3,19,35,-16,6.333\r\n",
      "Fairfield,0,3,24,40,-16,8.000\r\n",
      "Hampton,0,3,13,67,-54,4.333\r\n",
      "Manhattan,0,3,18,43,-25,6.000\r\n",
      "Mount St Mary's,0,3,18,45,-27,6.000\r\n",
      "St. Bonaventure,0,3,32,42,-10,10.667\r\n",
      "Utah,0,3,23,45,-22,7.667\r\n",
      "St. John's,0,4,38,61,-23,9.500\r\n"
     ]
    }
   ],
   "source": [
    "!cat ./data/teams.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e8ae53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849d76c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d2031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c09d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcf3c6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/stats1.csv.\n",
      "Data saved to ./data/stats2.csv.\n",
      "Data saved to ./data/stats3.csv.\n",
      "Data saved to ./data/stats4.csv.\n",
      "Data saved to ./data/stats5.csv.\n",
      "Data saved to ./data/stats6.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Load the HTML content from the new file\n",
    "with open('./data/raw_stats.html', 'r') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all instances of tables with the class \"table-striped\"\n",
    "tables = soup.find_all('table', class_='table-striped')\n",
    "\n",
    "for table_index, table in enumerate(tables, start=1):\n",
    "    # Extract the table headers, skipping the first header row as it is a category header\n",
    "    headers = [header.get('original-title', header.text).strip() for header in table.find_all('th')[1:]]\n",
    "\n",
    "    # Extract rows, starting from the second row to skip the header row\n",
    "    rows = []\n",
    "    for row in table.find_all('tr')[1:]:  # skipping the first row which is the header\n",
    "        cols = row.find_all('td')\n",
    "        if cols:\n",
    "            rows.append([col.text.strip() for col in cols])\n",
    "\n",
    "    # Create a DataFrame for the current table\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "    # Save the DataFrame to a CSV file, naming it based on the table index\n",
    "    csv_path = f'./data/stats{table_index}.csv'\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f'Data saved to {csv_path}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deb8d686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",Player,School,Total\r\n",
      "1,Joey Spallina,Syracuse,36\r\n",
      "2,Louis Perfetto,Boston U,28\r\n",
      "3,Brayden Mayea,High Point,27\r\n",
      "4,Luke Rusterucci,VMI,23\r\n",
      "5,Vince D'Alto,Boston U,21\r\n",
      "6,Jacob Angelus,Johns Hopkins,21\r\n",
      "7,Brennan O'Neill,Duke,21\r\n",
      "8,Jack VanOverbeke,High Point,20\r\n",
      "9,Josh Zawada,Duke,20\r\n",
      "10,John Madsen,Hofstra,19\r\n",
      "11,Matt Brandau,Yale,19\r\n",
      "12,Rory Jones,Hofstra,18\r\n",
      "13,Morgan O'Reilly,Sacred Heart,18\r\n",
      "14,Owen Hiltz,Syracuse,18\r\n",
      "15,Dalton Young,Richmond,17\r\n",
      "16,Nick Dupuis,Stony Brook,17\r\n",
      "17,Michael Boehm,Michigan,17\r\n",
      "18,Nick DeMaio,Towson,16\r\n",
      "19,Aidan O'Neil,Richmond,16\r\n",
      "20,Hunter Drouin,Colgate,15\r\n",
      "21,Sam King,Harvard,15\r\n",
      "22,Jake Stegman,Marquette,15\r\n",
      "23,Jack Rooney,Merrimack,15\r\n",
      "24,John Alie,Bellarmine,15\r\n",
      "25,Rory Connor,Colgate,15\r\n",
      "26,Bobby O'Grady,Marquette,15\r\n",
      "27,Brian Kelly,St. John's,14\r\n",
      "28,Ryan Bell,Providence,14\r\n",
      "29,Bo Lockwood,Michigan,14\r\n",
      "30,Christian Mulé,Syracuse,14\r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!cat ./data/stats1.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01f0d98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/stats_UnnamedTable.csv.\n",
      "Data saved to ./data/stats_UnnamedTable.csv.\n",
      "Data saved to ./data/stats_UnnamedTable.csv.\n",
      "Data saved to ./data/stats_UnnamedTable.csv.\n",
      "Data saved to ./data/stats_UnnamedTable.csv.\n",
      "Data saved to ./data/stats_UnnamedTable.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# file_names = [\n",
    "#     'statsTopScorers.csv',\n",
    "#     'statsAssistLeaders.csv',\n",
    "#     'statsGoalLeaders.csv',\n",
    "#     'statsPointsPerGameLeaders.csv',\n",
    "#     'statsTeamGoalsPerGame.csv',\n",
    "#     'statsTeamGoalsAgainstAvg.csv'\n",
    "# ]\n",
    "\n",
    "# Load the HTML content from the new file\n",
    "with open('./data/raw_stats.html', 'r') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all instances of tables with the class \"table-striped\"\n",
    "tables = soup.find_all('table', class_='table-striped')\n",
    "\n",
    "for table in tables:\n",
    "    # Attempt to find the table name from a preceding header\n",
    "    # This is a placeholder - you'll need to adjust the selection method based on your HTML's structure\n",
    "    table_name_element = table.find_previous_sibling(lambda tag: tag.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    table_name = table_name_element.text.strip() if table_name_element else 'UnnamedTable'\n",
    "    table_name = table_name.replace(' ', '_')  # Replace spaces with underscores for filename compatibility\n",
    "    \n",
    "    # Extract the table headers\n",
    "    headers = [header.get('original-title', header.text).strip() for header in table.find_all('th')[1:]]\n",
    "\n",
    "    # Extract rows\n",
    "    rows = []\n",
    "    for row in table.find_all('tr')[1:]:  # skipping the header row\n",
    "        cols = row.find_all('td')\n",
    "        if cols:\n",
    "            rows.append([col.text.strip() for col in cols])\n",
    "\n",
    "    # Create a DataFrame for the current table\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "    # Save the DataFrame to a CSV file, naming it based on the table name\n",
    "    csv_path = f'./data/stats_{table_name}.csv'\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f'Data saved to {csv_path}.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "681f900a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to ./data/statsTopScorers.csv.\n",
      "Data saved to ./data/statsAssistLeaders.csv.\n",
      "Data saved to ./data/statsGoalLeaders.csv.\n",
      "Data saved to ./data/statsPointsPerGameLeaders.csv.\n",
      "Data saved to ./data/statsTeamGoalsPerGame.csv.\n",
      "Data saved to ./data/statsTeamGoalsAgainstAvg.csv.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "file_names = [\n",
    "    'statsTopScorers.csv',\n",
    "    'statsAssistLeaders.csv',\n",
    "    'statsGoalLeaders.csv',\n",
    "    'statsPointsPerGameLeaders.csv',\n",
    "    'statsTeamGoalsPerGame.csv',\n",
    "    'statsTeamGoalsAgainstAvg.csv'\n",
    "]\n",
    "\n",
    "# Load the HTML content from the file\n",
    "with open('./data/raw_stats.html', 'r') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all instances of tables with the class \"table-striped\"\n",
    "tables = soup.find_all('table', class_='table-striped')\n",
    "\n",
    "# Iterate through each table and its corresponding filename\n",
    "for table, file_name in zip(tables, file_names):\n",
    "    # Extract the table headers\n",
    "    headers = [header.get('original-title', header.text).strip() for header in table.find_all('th')[1:]]\n",
    "\n",
    "    # Extract rows\n",
    "    rows = []\n",
    "    for row in table.find_all('tr')[1:]:  # Skipping the header row\n",
    "        cols = row.find_all('td')\n",
    "        if cols:\n",
    "            rows.append([col.text.strip() for col in cols])\n",
    "\n",
    "    # Create a DataFrame for the current table\n",
    "    df = pd.DataFrame(rows, columns=headers)\n",
    "\n",
    "    # Save the DataFrame to a CSV file, naming it based on the corresponding filename in the list\n",
    "    csv_path = f'./data/{file_name}'\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f'Data saved to {csv_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ebe0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.1 (pyenv)",
   "language": "python",
   "name": "python3.12.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
